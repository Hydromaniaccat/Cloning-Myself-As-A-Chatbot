{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run finetuning with my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import requests\n",
    "\n",
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_name = \"124M\"\n",
    "model_name = \"355M\"\n",
    "#model_name = \"774M\"\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "    print(f\"Downloading {model_name} model...\")\n",
    "    gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train model for me\n",
    "# name = 'george1'\n",
    "# file_name = \"../data/discord_merged_reformatted_reversed.txt\"\n",
    "\n",
    "# gpt2.finetune(sess,\n",
    "#               file_name,\n",
    "#               model_name=model_name,\n",
    "#               steps=1000,\n",
    "#               run_name=name,\n",
    "#               save_every=250,\n",
    "#               overwrite=True)   # steps is max number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train model for me\n",
    "# name = 'george2'\n",
    "# file_name = \"../data/discord_merged_reformatted.txt\"\n",
    "\n",
    "# gpt2.finetune(sess,\n",
    "#               file_name,\n",
    "#               model_name=model_name,\n",
    "#               steps=1000,\n",
    "#               run_name=name,\n",
    "#               save_every=250,\n",
    "#               overwrite=True)   # steps is max number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train model for me\n",
    "# name = 'george3'\n",
    "# file_name = \"../data/discord_merged_reformatted.txt\"\n",
    "\n",
    "# gpt2.finetune(sess,\n",
    "#               file_name,\n",
    "#               model_name=model_name,\n",
    "#               steps=4000,\n",
    "#               run_name=name,\n",
    "#               save_every=250,\n",
    "#               overwrite=True)   # steps is max number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train model for me\n",
    "# name = 'george4'\n",
    "# file_name = \"../data/discord_merged_reformatted.txt\"\n",
    "\n",
    "# gpt2.finetune(sess,\n",
    "#               file_name,\n",
    "#               model_name=model_name,\n",
    "#               steps=500,\n",
    "#               run_name=name,\n",
    "#               save_every=250,\n",
    "#               overwrite=True)   # steps is max number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\George\\Anaconda3\\envs\\tf15\\lib\\site-packages\\gpt_2_simple\\src\\sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\George\\Anaconda3\\envs\\tf15\\lib\\site-packages\\gpt_2_simple\\src\\memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
      "Instructions for updating:\n",
      "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
      "Loading checkpoint models\\355M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\355M\\model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 237903 tokens\n",
      "Training...\n",
      "Saving checkpoint\\george5\\model-0\n",
      "[1 | 9.30] loss=3.51 avg=3.51\n",
      "[2 | 10.23] loss=3.47 avg=3.49\n",
      "[3 | 11.15] loss=2.83 avg=3.27\n",
      "[4 | 12.06] loss=2.76 avg=3.14\n",
      "[5 | 12.97] loss=2.47 avg=3.00\n",
      "[6 | 13.88] loss=2.68 avg=2.95\n",
      "[7 | 14.81] loss=3.38 avg=3.01\n",
      "[8 | 15.72] loss=2.55 avg=2.95\n",
      "[9 | 16.64] loss=3.28 avg=2.99\n",
      "[10 | 17.55] loss=3.06 avg=3.00\n",
      "[11 | 18.46] loss=3.28 avg=3.02\n",
      "[12 | 19.38] loss=2.73 avg=3.00\n",
      "[13 | 20.29] loss=2.74 avg=2.98\n",
      "[14 | 21.20] loss=3.20 avg=2.99\n",
      "[15 | 22.11] loss=2.40 avg=2.95\n",
      "[16 | 23.02] loss=3.35 avg=2.98\n",
      "[17 | 23.94] loss=3.67 avg=3.02\n",
      "[18 | 24.85] loss=3.19 avg=3.03\n",
      "[19 | 25.77] loss=2.67 avg=3.01\n",
      "[20 | 26.69] loss=2.94 avg=3.01\n",
      "[21 | 27.61] loss=3.06 avg=3.01\n",
      "[22 | 28.53] loss=2.94 avg=3.01\n",
      "[23 | 29.45] loss=2.74 avg=2.99\n",
      "[24 | 30.36] loss=2.85 avg=2.99\n",
      "[25 | 31.28] loss=2.52 avg=2.97\n",
      "[26 | 32.19] loss=2.53 avg=2.95\n",
      "[27 | 33.11] loss=3.30 avg=2.96\n",
      "[28 | 34.02] loss=3.24 avg=2.97\n",
      "[29 | 34.94] loss=2.99 avg=2.97\n",
      "[30 | 35.86] loss=2.97 avg=2.97\n",
      "[31 | 36.77] loss=2.85 avg=2.97\n",
      "[32 | 37.68] loss=3.21 avg=2.98\n",
      "[33 | 38.60] loss=3.46 avg=2.99\n",
      "[34 | 39.52] loss=2.84 avg=2.99\n",
      "[35 | 40.44] loss=2.60 avg=2.98\n",
      "[36 | 41.36] loss=3.40 avg=2.99\n",
      "[37 | 42.27] loss=2.26 avg=2.97\n",
      "[38 | 43.19] loss=2.94 avg=2.97\n",
      "[39 | 44.10] loss=2.13 avg=2.94\n",
      "[40 | 45.02] loss=3.15 avg=2.95\n",
      "[41 | 45.93] loss=2.88 avg=2.94\n",
      "[42 | 46.85] loss=2.35 avg=2.93\n",
      "[43 | 47.76] loss=3.36 avg=2.94\n",
      "[44 | 48.68] loss=2.95 avg=2.94\n",
      "[45 | 49.60] loss=2.36 avg=2.92\n",
      "[46 | 50.51] loss=2.84 avg=2.92\n",
      "[47 | 51.43] loss=3.40 avg=2.93\n",
      "[48 | 52.34] loss=3.33 avg=2.94\n",
      "[49 | 53.26] loss=2.89 avg=2.94\n",
      "[50 | 54.17] loss=2.85 avg=2.94\n",
      "[51 | 55.09] loss=2.71 avg=2.94\n",
      "[52 | 56.01] loss=2.87 avg=2.93\n",
      "[53 | 56.92] loss=3.03 avg=2.94\n",
      "[54 | 57.84] loss=3.08 avg=2.94\n",
      "[55 | 58.75] loss=3.32 avg=2.95\n",
      "[56 | 59.67] loss=2.74 avg=2.94\n",
      "[57 | 60.59] loss=3.08 avg=2.95\n",
      "[58 | 61.51] loss=3.18 avg=2.95\n",
      "[59 | 62.43] loss=2.55 avg=2.94\n",
      "[60 | 63.34] loss=2.68 avg=2.94\n",
      "[61 | 64.26] loss=2.52 avg=2.93\n",
      "[62 | 65.18] loss=2.41 avg=2.92\n",
      "[63 | 66.10] loss=2.72 avg=2.91\n",
      "[64 | 67.02] loss=2.55 avg=2.90\n",
      "[65 | 67.95] loss=3.09 avg=2.91\n",
      "[66 | 68.87] loss=2.82 avg=2.91\n",
      "[67 | 69.79] loss=2.69 avg=2.90\n",
      "[68 | 70.71] loss=2.97 avg=2.90\n",
      "[69 | 71.63] loss=2.35 avg=2.89\n",
      "[70 | 72.55] loss=2.63 avg=2.89\n",
      "[71 | 73.47] loss=2.72 avg=2.88\n",
      "[72 | 74.38] loss=3.05 avg=2.89\n",
      "[73 | 75.31] loss=3.04 avg=2.89\n",
      "[74 | 76.23] loss=2.52 avg=2.88\n",
      "[75 | 77.15] loss=2.89 avg=2.88\n",
      "[76 | 78.07] loss=2.42 avg=2.87\n",
      "[77 | 78.99] loss=3.17 avg=2.88\n",
      "[78 | 79.91] loss=2.66 avg=2.88\n",
      "[79 | 80.84] loss=3.34 avg=2.88\n",
      "[80 | 81.76] loss=2.44 avg=2.88\n",
      "[81 | 82.68] loss=2.76 avg=2.87\n",
      "[82 | 83.60] loss=2.37 avg=2.87\n",
      "[83 | 84.52] loss=2.50 avg=2.86\n",
      "[84 | 85.43] loss=2.30 avg=2.85\n",
      "[85 | 86.36] loss=2.65 avg=2.85\n",
      "[86 | 87.28] loss=2.75 avg=2.84\n",
      "[87 | 88.20] loss=3.03 avg=2.85\n",
      "[88 | 89.13] loss=3.21 avg=2.85\n",
      "[89 | 90.05] loss=2.95 avg=2.86\n",
      "[90 | 90.97] loss=2.73 avg=2.85\n",
      "[91 | 91.90] loss=2.84 avg=2.85\n",
      "[92 | 92.82] loss=2.76 avg=2.85\n",
      "[93 | 93.74] loss=2.68 avg=2.85\n",
      "[94 | 94.66] loss=3.03 avg=2.85\n",
      "[95 | 95.59] loss=2.45 avg=2.84\n",
      "[96 | 96.51] loss=2.91 avg=2.85\n",
      "[97 | 97.44] loss=2.98 avg=2.85\n",
      "[98 | 98.37] loss=2.33 avg=2.84\n",
      "[99 | 99.29] loss=2.51 avg=2.83\n",
      "[100 | 100.22] loss=2.15 avg=2.82\n",
      "======== SAMPLE 1 ========\n",
      " like 'Who's gonna be best at the competition?' It's not really a competition. What I'd like to see is you guys come out, talk as one, how's your lives and the people you care for going? It's hard to see people when the screen is on. I'd like to see the pros but I don't know much. <|endoftext|>\n",
      "Is that not a good example?\n",
      "<|BrainSquid|> Well, it has a bunch of people. <|endoftext|>\n",
      "are you a pro at what I need it to be?\n",
      "<|BrainSquid|> Well, but what do these buttons do anyway, I'm in the middle of editing. <|endoftext|>\n",
      "Is it really? do I have the right software? I have like 3.5 days of free time on my hands.\n",
      "<|BrainSquid|> The only good pro it might be that all of your friends are at the event. <|endoftext|>\n",
      "I think most of them aren't here today because they're on a trip.\n",
      "<|BrainSquid|> I was talking with a guy today. He's been at it for like 5 hours. He said he'd be going home soon. <|endoftext|>\n",
      "what if I take my place? does that make anyone happy? do you have any food that's not cheap? should I order from there, maybe you can take a pic of me ordering from there.\n",
      "<|BrainSquid|> Then I'll tell you. But this isn't your game. <|endoftext|>\n",
      "I don't see everything that's available to view when all you see is a graph from an algorithm.\n",
      "<|BrainSquid|> So far I haven't seen anything else. <|endoftext|>\n",
      "My guess is that it'll be like this.\n",
      "<|BrainSquid|> In the next 5 or 6 days. <|endoftext|>\n",
      "Can't you just show me the chart? I have like 5 days before I go to bed. Why don't you show me what it shows?\n",
      "<|BrainSquid|> I can do that later in the evening. <|endoftext|>\n",
      "You can't. I have homework to do and I need to go to the park or something like that.\n",
      "<|BrainSquid|> But I'm at 11:30. <|endoftext|>\n",
      "that's how I see it.\n",
      "<|BrainSquid|> But why are you at 11:30 instead of I just went to park? <|endoftext|>\n",
      "No but I'm not there yet.\n",
      "<|BrainSquid|> I need to go get pizza. <|endoftext|>\n",
      "I'm not saying that I don't need a pizza. I just think that we're more important without it.\n",
      "<|BrainSquid|> Oh no you do not. <|endoftext|>\n",
      "But why it's important without me doing it.\n",
      "<|BrainSquid|> So that's your reason for not being present. <|endoftext|>\n",
      "but I'm still trying to figure that one out.\n",
      "<|BrainSquid|> What was it that I wanted to do but then later in the evening when I'm done I can't be there. <|endoftext|>\n",
      "I think I need your help.\n",
      "<|BrainSquid|> I just need to show you why it's better to be absent. <|endoftext|>\n",
      "I don't really.\n",
      "<|BrainSquid|> What do you mean with my help. <|endoftext|>\n",
      "Why would it be better?\n",
      "<|BrainSquid|> Did you like the music? <|endoftext|>\n",
      "The music sucks.\n",
      "<|BrainSquid|> But I like the fact that you don't want to go to the park for a few hours. <|endoftext|>\n",
      "That's what I said to alex before we went because I was in a bad mood. But that's not all.\n",
      "<|BrainSquid|> Oh my god your not even close. <|endoftext|>\n",
      "The park isn't even open.\n",
      "<|BrainSquid|> Yeah, but do you recognize the sign that's one sign that goes to the park. <|endoftext|>\n",
      "I had a look over it and it's actually not what it says. It's a good sign, though. So yeah, but there's enough text in the picture and even a better looking version of the same picture on the sign isn't really that bad.\n",
      "<|BrainSquid|> I know what you mean. <|endoftext\n",
      "\n",
      "[101 | 123.49] loss=2.76 avg=2.82\n",
      "[102 | 124.41] loss=3.18 avg=2.83\n",
      "[103 | 125.34] loss=2.75 avg=2.83\n",
      "[104 | 126.25] loss=2.33 avg=2.82\n",
      "[105 | 127.18] loss=2.54 avg=2.82\n",
      "[106 | 128.10] loss=2.75 avg=2.81\n",
      "[107 | 129.03] loss=2.42 avg=2.81\n",
      "[108 | 129.95] loss=3.34 avg=2.82\n",
      "[109 | 130.87] loss=2.71 avg=2.81\n",
      "[110 | 131.79] loss=3.07 avg=2.82\n",
      "[111 | 132.72] loss=3.13 avg=2.82\n",
      "[112 | 133.65] loss=2.76 avg=2.82\n",
      "[113 | 134.57] loss=2.80 avg=2.82\n",
      "[114 | 135.50] loss=2.50 avg=2.82\n",
      "[115 | 136.43] loss=2.79 avg=2.82\n",
      "[116 | 137.35] loss=2.75 avg=2.82\n",
      "[117 | 138.27] loss=3.13 avg=2.82\n",
      "[118 | 139.20] loss=2.77 avg=2.82\n",
      "[119 | 140.12] loss=2.33 avg=2.81\n",
      "[120 | 141.05] loss=2.82 avg=2.81\n",
      "[121 | 141.98] loss=1.86 avg=2.80\n",
      "[122 | 142.91] loss=2.78 avg=2.80\n",
      "[123 | 143.84] loss=2.45 avg=2.79\n",
      "[124 | 144.79] loss=2.91 avg=2.80\n",
      "[125 | 145.67] loss=2.38 avg=2.79\n",
      "[126 | 146.57] loss=3.33 avg=2.80\n",
      "[127 | 147.51] loss=3.10 avg=2.80\n",
      "[128 | 148.68] loss=2.82 avg=2.80\n",
      "[129 | 149.85] loss=2.44 avg=2.80\n",
      "[130 | 150.77] loss=3.00 avg=2.80\n",
      "[131 | 151.89] loss=2.10 avg=2.79\n",
      "[132 | 153.39] loss=3.05 avg=2.79\n",
      "[133 | 154.43] loss=3.04 avg=2.80\n",
      "[134 | 155.58] loss=2.56 avg=2.79\n",
      "[135 | 156.70] loss=2.82 avg=2.79\n",
      "[136 | 157.87] loss=2.88 avg=2.80\n",
      "[137 | 159.05] loss=2.46 avg=2.79\n",
      "[138 | 160.17] loss=2.22 avg=2.78\n",
      "[139 | 161.30] loss=2.35 avg=2.78\n",
      "[140 | 162.43] loss=2.43 avg=2.77\n",
      "[141 | 163.58] loss=2.79 avg=2.77\n",
      "[142 | 164.71] loss=2.64 avg=2.77\n",
      "[143 | 165.84] loss=3.44 avg=2.78\n",
      "[144 | 166.95] loss=3.32 avg=2.79\n",
      "[145 | 168.07] loss=2.83 avg=2.79\n",
      "[146 | 169.20] loss=2.14 avg=2.78\n",
      "[147 | 170.30] loss=2.36 avg=2.77\n",
      "[148 | 171.22] loss=2.77 avg=2.77\n",
      "[149 | 172.45] loss=3.06 avg=2.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150 | 173.72] loss=2.93 avg=2.78\n",
      "[151 | 174.68] loss=3.05 avg=2.78\n",
      "[152 | 175.63] loss=2.12 avg=2.77\n",
      "[153 | 176.57] loss=2.68 avg=2.77\n",
      "[154 | 177.48] loss=2.01 avg=2.76\n",
      "[155 | 178.41] loss=2.86 avg=2.76\n",
      "[156 | 179.34] loss=3.01 avg=2.77\n",
      "[157 | 180.28] loss=2.26 avg=2.76\n",
      "[158 | 181.21] loss=3.44 avg=2.77\n",
      "[159 | 182.12] loss=1.84 avg=2.76\n",
      "[160 | 183.07] loss=3.48 avg=2.77\n",
      "[161 | 184.01] loss=3.28 avg=2.77\n",
      "[162 | 184.95] loss=2.66 avg=2.77\n",
      "[163 | 185.86] loss=2.35 avg=2.77\n",
      "[164 | 186.78] loss=2.88 avg=2.77\n",
      "[165 | 187.69] loss=1.44 avg=2.75\n",
      "[166 | 188.62] loss=2.48 avg=2.75\n",
      "[167 | 189.53] loss=2.83 avg=2.75\n",
      "[168 | 190.43] loss=2.81 avg=2.75\n",
      "[169 | 191.34] loss=2.90 avg=2.75\n",
      "[170 | 192.25] loss=2.58 avg=2.75\n",
      "[171 | 193.14] loss=2.29 avg=2.74\n",
      "[172 | 194.06] loss=2.43 avg=2.74\n",
      "[173 | 194.98] loss=2.00 avg=2.73\n",
      "[174 | 195.89] loss=2.74 avg=2.73\n",
      "[175 | 196.80] loss=2.40 avg=2.73\n",
      "[176 | 197.70] loss=1.95 avg=2.72\n",
      "[177 | 198.61] loss=2.23 avg=2.71\n",
      "[178 | 199.52] loss=2.70 avg=2.71\n",
      "[179 | 200.43] loss=2.47 avg=2.71\n",
      "[180 | 201.36] loss=3.01 avg=2.71\n",
      "[181 | 202.27] loss=2.73 avg=2.71\n",
      "[182 | 203.18] loss=2.68 avg=2.71\n",
      "[183 | 204.10] loss=2.13 avg=2.71\n",
      "[184 | 205.02] loss=2.46 avg=2.70\n",
      "[185 | 205.94] loss=1.81 avg=2.69\n",
      "[186 | 206.86] loss=1.99 avg=2.68\n",
      "[187 | 207.77] loss=1.85 avg=2.67\n",
      "[188 | 208.69] loss=2.20 avg=2.67\n",
      "[189 | 209.59] loss=2.45 avg=2.67\n",
      "[190 | 210.50] loss=2.63 avg=2.67\n",
      "[191 | 211.41] loss=1.97 avg=2.66\n",
      "[192 | 212.32] loss=3.23 avg=2.66\n",
      "[193 | 213.21] loss=2.77 avg=2.67\n",
      "[194 | 214.10] loss=2.19 avg=2.66\n",
      "[195 | 215.01] loss=1.90 avg=2.65\n",
      "[196 | 215.91] loss=2.25 avg=2.65\n",
      "[197 | 216.81] loss=2.23 avg=2.64\n",
      "[198 | 217.72] loss=2.47 avg=2.64\n",
      "[199 | 218.64] loss=2.68 avg=2.64\n",
      "[200 | 219.55] loss=2.43 avg=2.64\n",
      "======== SAMPLE 1 ========\n",
      " hard. (That one's for the actual games) <|endoftext|>\n",
      "and it's fun. and you need to build a team. <|endoftext|>\n",
      "I want to be able to tell if I'm being watched.\n",
      "<|BrainSquid|> why are you so quiet. the game will be out as soon as the team has to train tomorrow so that's the plan. <|endoftext|>\n",
      "But I'm so damn quiet that it starts a debate. it's because I'm not used to having to keep on voice in chat. It's so damn silence that I have to check it to make sure that I can hear. Then I just switch to my preferred channel when watching Dota 2, or whenever teamfights happen.\n",
      "<|BrainSquid|> I dont want to be a useless braindead. but can you not interrupt what I'm saying on stream? <|endoftext|>\n",
      "I just meant to get up to watch them play. and you interrupted. I'm like a fish in water. It seems like there's never enough. So I'll stay away from the other chat. I thought things were getting better?\n",
      "<|BrainSquid|> I think they got better before I joined. I was looking forward to hearing all about it, but now the thing that made it better is all gone. the only thing that's missing is a strong second wave. and what did that do? <|endoftext|>\n",
      "And it'll still be an average wave.\n",
      "<|BrainSquid|> the second wave isn't that big. and the wave of the future isn't as high-risk or as low-risk. and most of the wave that broke a few weeks ago will end today. I can only assume that it was delayed because it was too strong. and then it broke again, maybe because it was stronger. and the next wave is going to be strong, I guess. and one more, I think is pretty much guaranteed to break today. so I was just speculating. <|endoftext|>\n",
      "Did we watch the whole episode?\n",
      "<|BrainSquid|> I saw it. and i really liked how Alex got so defensive. but how the fuck did it get to the point where he's so defensive? but he wasn't defending it then. no, what Alex does is when someone is harassing him, he acts like he's the one losing. <|endoftext|>\n",
      "I just wanted to point out that it's also pretty damn stupid in retrospect. I should have gotten a better sense of that earlier.\n",
      "<|BrainSquid|> I thought Alex was more articulate. but I was wrong. you should have seen him. he was speaking with no context. and he seemed confused a large number of times. <|endoftext|>\n",
      "I don't want stupid. I just want people to be logical enough for the rest of the argument to connect.\n",
      "<|BrainSquid|> I like when people have logical arguments because it makes the rest of the argument sound bad. i think he went one direction but actually went the other way. and that was really annoying. but he ended up being right. and he was very clear what he was saying. <|endoftext|>\n",
      "Oh well, he wasn't talking to you, so it's not his intent.\n",
      "<|BrainSquid|> He was trying to rationalize it later, and we all know what happened that whole situation. which is that he was being unreasonable. and he should have realized how stupid we are. but he didn't so instead, he doubled down on what he said. and basically kept it going. and then eventually I realized that we all understand each other better then. and he was just being logical. after realizing he screwed us all over. <|endoftext|>\n",
      "So that's a happy ending.\n",
      "<|BrainSquid|> We all want the best of life but we dont know when, or if we ever will, know that. <|endoftext|>\n",
      "I don't want to be a martyr. and I think all of this was just my imagination. I'm just not going to be a martyr. but when it turns out that I don't deserve to be, then I'll be one.\n",
      "<|BrainSquid|> So why is he going up in fireballs now? does he have some reason to save the day? oh well at least he wasn't a martyr. I think that's all a big misunderstanding is about. <|endoftext|>\n",
      "It's a rationalization. It's probably my own subconscious that wants it to look good.\n",
      "<|BrainSquid|> It had better not have been a martyr. and maybe the world doesn't need martyrs anymore. in any case... he was a hero. but who cares so much\n",
      "\n",
      "[201 | 240.17] loss=2.74 avg=2.64\n",
      "[202 | 241.08] loss=2.69 avg=2.64\n",
      "[203 | 242.00] loss=3.01 avg=2.64\n",
      "[204 | 242.90] loss=2.46 avg=2.64\n",
      "[205 | 243.81] loss=3.05 avg=2.65\n",
      "[206 | 244.70] loss=2.33 avg=2.64\n",
      "[207 | 245.59] loss=2.62 avg=2.64\n",
      "[208 | 246.49] loss=3.03 avg=2.65\n",
      "[209 | 247.40] loss=3.09 avg=2.65\n",
      "[210 | 248.31] loss=2.18 avg=2.65\n",
      "[211 | 249.22] loss=2.41 avg=2.64\n",
      "[212 | 250.14] loss=2.65 avg=2.64\n",
      "[213 | 251.05] loss=2.28 avg=2.64\n",
      "[214 | 251.97] loss=3.20 avg=2.65\n",
      "[215 | 252.88] loss=2.58 avg=2.65\n",
      "[216 | 253.77] loss=2.67 avg=2.65\n",
      "[217 | 254.67] loss=2.59 avg=2.65\n",
      "[218 | 255.58] loss=2.42 avg=2.64\n",
      "[219 | 256.49] loss=1.91 avg=2.63\n",
      "[220 | 257.41] loss=2.03 avg=2.63\n",
      "[221 | 258.33] loss=2.66 avg=2.63\n",
      "[222 | 259.25] loss=1.42 avg=2.61\n",
      "[223 | 260.15] loss=2.17 avg=2.61\n",
      "[224 | 261.08] loss=2.92 avg=2.61\n",
      "[225 | 262.01] loss=2.70 avg=2.61\n",
      "[226 | 262.94] loss=2.65 avg=2.61\n",
      "[227 | 263.87] loss=1.67 avg=2.60\n",
      "[228 | 264.83] loss=1.61 avg=2.59\n",
      "[229 | 265.74] loss=2.73 avg=2.59\n",
      "[230 | 266.65] loss=1.60 avg=2.58\n",
      "[231 | 267.64] loss=1.81 avg=2.57\n",
      "[232 | 269.12] loss=2.70 avg=2.58\n",
      "[233 | 270.13] loss=2.60 avg=2.58\n",
      "[234 | 271.01] loss=1.57 avg=2.57\n",
      "[235 | 272.06] loss=2.52 avg=2.56\n",
      "[236 | 273.39] loss=2.94 avg=2.57\n",
      "[237 | 274.59] loss=2.18 avg=2.56\n",
      "[238 | 275.75] loss=2.77 avg=2.57\n",
      "[239 | 276.97] loss=2.94 avg=2.57\n",
      "[240 | 278.14] loss=2.39 avg=2.57\n",
      "[241 | 279.28] loss=3.12 avg=2.57\n",
      "[242 | 280.46] loss=2.09 avg=2.57\n",
      "[243 | 281.63] loss=2.19 avg=2.57\n",
      "[244 | 282.76] loss=3.02 avg=2.57\n",
      "[245 | 283.90] loss=2.67 avg=2.57\n",
      "[246 | 285.03] loss=2.45 avg=2.57\n",
      "[247 | 286.16] loss=3.32 avg=2.58\n",
      "[248 | 287.38] loss=2.18 avg=2.57\n",
      "[249 | 288.65] loss=2.63 avg=2.57\n",
      "[250 | 289.90] loss=2.37 avg=2.57\n",
      "Saving checkpoint\\george5\\model-250\n",
      "WARNING:tensorflow:From C:\\Users\\George\\Anaconda3\\envs\\tf15\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "#Train model for me\n",
    "name = 'george5'\n",
    "file_name = \"../data/discord_merged_reformatted.txt\"\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "              file_name,\n",
    "              model_name=model_name,\n",
    "              steps=250,\n",
    "              run_name=name,\n",
    "              save_every=250,\n",
    "              overwrite=True)   # steps is max number of training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Chuck Norris can believe it's not butter.\n",
      " I know, I know. I mean, I know. I'm just saying, I don't know if I want to. \n",
      ">>> Do you believe in butter?\n",
      " I don't think so.  I don't think so.  I don't think so.  I don't think so. \n",
      ">>> Tell me about Trump\n",
      " I don't think he's a conservative. \n",
      ">>> That's what I've been saying. He's definitely an independent\n",
      " but he also is pro-choice. \n",
      ">>> I know. I hate that he has to pretend to be a Republican in order to get elected.\n",
      " He's the only person who can actually win the general. \n"
     ]
    }
   ],
   "source": [
    "k=50\n",
    "p=.9\n",
    "t=.6\n",
    "max_history=4\n",
    "\n",
    "start_token = '<|BrainSquid|>'\n",
    "history = []\n",
    "while True:\n",
    "    raw_text = input(\">>> \")\n",
    "    while not raw_text:\n",
    "        raw_text = input(\">>> \")\n",
    "    #Append input to history\n",
    "    history.append(raw_text)\n",
    "    out_ids = gpt2.generate(sess, run_name=name, prefix=' '.join(history) + \" \" + start_token, truncate=\"<|endoftext|>\", temperature=t, top_k=k, top_p=p, include_prefix=False, return_as_list=True)\n",
    "    #Append output to history\n",
    "    output = ' '.join(out_ids)\n",
    "    output = output.split(start_token)[0]\n",
    "\n",
    "    if sum([x in output for x in history]) > 2:#== len(history):\n",
    "        #retry\n",
    "        error_count += 1\n",
    "        print('ERROR ' + str(error_count), end='\\r')\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    history.append(output)\n",
    "    history = history[-(2*max_history+1):]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 tensorflow==1.5",
   "language": "python",
   "name": "tf15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
